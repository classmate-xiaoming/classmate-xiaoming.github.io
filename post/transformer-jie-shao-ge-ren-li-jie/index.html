<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Transformer介绍（个人理解） | 小明同学的博客</title>
<link rel="shortcut icon" href="https://classmate-xiaoming.github.io/favicon.ico?v=1607568973377">
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.3.0/fonts/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://classmate-xiaoming.github.io/styles/main.css">
<link rel="alternate" type="application/atom+xml" title="Transformer介绍（个人理解） | 小明同学的博客 - Atom Feed" href="https://classmate-xiaoming.github.io/atom.xml">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">



    <meta name="description" content="Transformer介绍（个人理解）
前言
在阅读了谷歌attention is all you need和一篇很棒的解释文章后对transformer应用注意力机制有了一定的理解，利用注意力机制代替传统的RNN模型更好的利用了上下文的信..." />
    <meta name="keywords" content="NLP,人工智能,算法" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
    <script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
  </head>
  <body>
    <div class="main">
      <div class="main-content">
        <div class="site-header">
  <a href="https://classmate-xiaoming.github.io">
  <img class="avatar" src="https://classmate-xiaoming.github.io/images/avatar.png?v=1607568973377" alt="">
  </a>
  <h1 class="site-title">
    小明同学的博客
  </h1>
  <p class="site-description">
    小白学习日记
  </p>
  <div class="menu-container">
    
      
        <a href="/" class="menu">
          首页
        </a>
      
    
      
        <a href="/archives" class="menu">
          归档
        </a>
      
    
      
        <a href="/tags" class="menu">
          标签
        </a>
      
    
      
        <a href="/post/about" class="menu">
          关于
        </a>
      
    
  </div>
  <div class="social-container">
    
      
    
      
    
      
    
      
    
      
    
  </div>
</div>

        <div class="post-detail">
          <article class="post">
            <h2 class="post-title">
              Transformer介绍（个人理解）
            </h2>
            <div class="post-info">
              <span>
                2020-12-10
              </span>
              <span>
                6 min read
              </span>
              
                <a href="https://classmate-xiaoming.github.io/tag/DJbPw7VzT/" class="post-tag">
                  # NLP
                </a>
              
                <a href="https://classmate-xiaoming.github.io/tag/vEcOeC5EW/" class="post-tag">
                  # 人工智能
                </a>
              
                <a href="https://classmate-xiaoming.github.io/tag/sJ1vXf-AFX/" class="post-tag">
                  # 算法
                </a>
              
            </div>
            
              <img class="post-feature-image" src="https://raw.githubusercontent.com/classmate-xiaoming/classmate-xiaoming.github.io/master/img/20201210100523.png" alt="">
            
            <div class="post-content-wrapper">
              <div class="post-content">
                <h1 id="transformer介绍个人理解">Transformer介绍（个人理解）</h1>
<h2 id="前言">前言</h2>
<p>在阅读了谷歌attention is all you need和一篇很棒的解释文章后对transformer应用注意力机制有了一定的理解，利用注意力机制代替传统的RNN模型更好的利用了上下文的信息。</p>
<p>文章链接：</p>
<p><a href="https://jalammar.github.io/illustrated-transformer/"><strong>The Illustrated Transformer</strong></a></p>
<p><a href="https://arxiv.org/abs/1706.03762"><strong>Attention Is All You Need</strong></a></p>
<p><a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">Transformer: A Novel Neural Network Architecture for Language Understanding</a></p>
<h2 id="模型简介">模型简介</h2>
<img src="https://raw.githubusercontent.com/classmate-xiaoming/classmate-xiaoming.github.io/master/img/20201210100523.png" style="margin: auto;" />
<p>模型采用经典的编码器-解码器结构。</p>
<h3 id="编码器">编码器</h3>
<p>编码器首先流入一个self-attention层，该层帮助每个单词获得其前后单词的信息，再将attention层的输出输入一个前向神经网络中，并在每层之间引入残差连接来稳定梯度。</p>
<h3 id="解码器">解码器</h3>
<p>解码器的构成与编码器类似，同样采用了self-attention层和前向神经网络层，但在中间加入了一层获取编码器信息的attention层，与seq2seq模型中注意力机制类似，让解码器可以关注到每层编码器的信息，而不只是编码器最后的输出。</p>
<h2 id="向量化">向量化</h2>
<p>在输入时transformer采用和大部分NLP应用同样的方式采用embedding将单词转化为词向量，输入到编码器中，在本文中采用的为512维的词向量，为了保证进行残差计算，编码器输出的维度都维持在512维。</p>
<img src="https://raw.githubusercontent.com/classmate-xiaoming/classmate-xiaoming.github.io/master/img/20201210100643.png" style="margin: auto;" />
<p>在self-attention层时，每个单词的路径是两两相关的，在前向网络时每个路径则没有相关的依赖，这样保证这些路径可以并行执行。</p>
<h2 id="self-attention">Self-Attention</h2>
<p>self-attention是整个transfromer模型最关键的部分，self-attention对之前的attention机制的变种，加入了多头机制和缩放因子。</p>
<h3 id="self-attention作用">Self-Attention作用</h3>
<p>Self-Attention帮助模型更好的联系上下文，在机器翻译中，如果要翻译“The animal didn’t cross the street because it was too tired”。句子中&quot;it&quot;指的是什么呢？“it&quot;指的是&quot;street” 还是“animal”？对人来说很简单的问题，但是对算法而言并不简单。</p>
<p>当处理单词“it”时，self-attention会将将“it”和“animal”联系起来，把相关词的理解编码到当前词之中。</p>
<img src="https://raw.githubusercontent.com/classmate-xiaoming/classmate-xiaoming.github.io/master/img/20201210100726.png" style="margin: auto;" />
<h3 id="self-attention计算过程">Self-Attention计算过程</h3>
<p><strong>第一步</strong>，计算对应的query/key/value 矩阵，将所有的输入词向量合并为矩阵X，并将其分别乘以权重矩阵</p>
<img src="https://raw.githubusercontent.com/classmate-xiaoming/classmate-xiaoming.github.io/master/img/20201210100749.png" style="margin: auto;" />
<p><u>个人理解：attention机制利用query矩阵去检索所需要部分的信息，通过点积的方式和key矩阵进行匹配，找到相关性更高的key并利用key-value键值对找到所需的value，来完成注意力机制。</u></p>
<p>**第二步：**Q矩阵点乘K矩阵的转置得到匹配后的矩阵，**此处Self-Attention引入了缩放因子，点积后的矩阵除以<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{d_k}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.18278000000000005em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.85722em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.81722em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,
-10,-9.5,-14c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54c44.2,-33.3,65.8,
-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10s173,378,173,378c0.7,0,
35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429c69,-144,104.5,-217.7,106.5,
-221c5.3,-9.3,12,-14,20,-14H400000v40H845.2724s-225.272,467,-225.272,467
s-235,486,-235,486c-2.7,4.7,-9,7,-19,7c-6,0,-10,-1,-12,-3s-194,-422,-194,-422
s-65,47,-65,47z M834 80H400000v40H845z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.18278000000000005em;"><span></span></span></span></span></span></span></span></span>,来使梯度更加稳定，通过softmax操作归一化表示对应词的相关程度，**再用得到的矩阵点乘V矩阵得到每个词的Value。</p>
<img src="https://raw.githubusercontent.com/classmate-xiaoming/classmate-xiaoming.github.io/master/img/20201210101026.png" style="margin: auto;" />
<h3 id="self-attention多头机制">Self-Attention多头机制</h3>
<p>如果只采用一组query/key/value矩阵最后得到的结果一般都会更关注所处理的词本身，如输入“apple tree”时，apple和tree对应的输出z都会更多取自自身的value，为解决上述问题，Self-Attention引入了<strong>多头机制</strong>来解决问题。</p>
<p><strong>多头机制</strong>使用多组query/key/value矩阵（文中使用八组），每一组进行随机的初始化。</p>
<img src="https://raw.githubusercontent.com/classmate-xiaoming/classmate-xiaoming.github.io/master/img/20201210101108.png" style="margin: auto;" />
<p>文中使用八组矩阵，得到不同的八个输出z值矩阵，实现了每个z值矩阵关注句子的不同部分并根据相关性确定各个部分的value，实现了对上下文的有效利用。</p>
<img src="https://raw.githubusercontent.com/classmate-xiaoming/classmate-xiaoming.github.io/master/img/20201210101147.png" style="margin: auto;" />
<p>但是前向神经网络不能直接接收八个矩阵，而是希望接收一个矩阵，所以需要将八个矩阵合并为一个矩阵</p>
<img src="https://raw.githubusercontent.com/classmate-xiaoming/classmate-xiaoming.github.io/master/img/20201210101223.png" style="margin: auto;" />
<p>在加入多头机制时，可以看到进行翻译后“it”集中于“the animal”和“tired”上，可见模型集中了两者的信息，对“it”进行合成处理。</p>
<img src="https://raw.githubusercontent.com/classmate-xiaoming/classmate-xiaoming.github.io/master/img/20201210101309.png" style="margin: auto;" />
<h3 id="位置编码">位置编码</h3>
<p>可以注意到，再利用attention方法时，我们虽然很好的获取了处理单词的上下文信息但是我们忽略了文本的位置信息，为了利用位置信息我们对embedding的词向量加上位置编码来提供词向量间的距离。</p>
<p>以四维向量为例的位置编码：</p>
<img src="https://raw.githubusercontent.com/classmate-xiaoming/classmate-xiaoming.github.io/master/img/20201210101357.png" style="margin: auto;" />
<p>在文中采用的向量是512维，位置向量也对应为512维，每个值都在【-1，1】间，利用涂色可视化：</p>
<img src="https://raw.githubusercontent.com/classmate-xiaoming/classmate-xiaoming.github.io/master/img/20201210101433.png" style="margin: auto;" />
<p>可以看到位置编码具有一定的周期性，本文采用的位置编码公式如下：</p>
<img src="https://raw.githubusercontent.com/classmate-xiaoming/classmate-xiaoming.github.io/master/img/20201210101644.png" style="margin: auto;" />
<p>公式的设计十分巧妙，利用了三角函数的周期性，建模了词之间的位置关系，好过直接利用词的绝对位置的方式，修补了其只能建模有限长度的序列的缺点，同时利用了三角函数的值域为【-1，1】，可以很好的配合embedding的向量。</p>
<h2 id="残差">残差</h2>
<p>编码器和解码器在每个子层中都有残差连接并紧跟着layer-normalization，加快模型收敛，具体过程如下：</p>
<img src="https://raw.githubusercontent.com/classmate-xiaoming/classmate-xiaoming.github.io/master/img/20201210101723.png" style="margin: auto;" />
<h2 id="解码器-2">解码器</h2>
<p>每个解码器模块接收编码器输出的K和V矩阵，作为中间的“encoder-decoder attention”层的输入，所进行的过程如下图：</p>
<figure data-type="image" tabindex="1"><img src="https://classmate-xiaoming.github.io/post-images/1607568276895.gif" alt="" loading="lazy"></figure>
<p>解码器部分的·self-attention层预编码器中的不同，因为编码器部分只会关注早于当前位置的输出，得到的输出作为Q矩阵输入到“encoder-decoder attention”层，接收编码器的输出K,V矩阵作为K,V矩阵。</p>
<h2 id="优化算法">优化算法</h2>
<p>在训练时采用了Adam方法，本文利用了一种较warm up的学习率调节方式，公示如下图：</p>
<img src="https://raw.githubusercontent.com/classmate-xiaoming/classmate-xiaoming.github.io/master/img/20201210101918.png" style="margin: auto;" />
<p>采用一个warmup-steps的超参，在训练刚开始进行时，第二项决定学习率，此时学习率上升，当训练进行到一定程度时，第一项决定学习率，学习率开始下降，这样先上升再下降的学习率有利于让模型快速收敛。</p>
<h2 id="正则化方法">正则化方法</h2>
<p>模型也采用了两种正则化方法，一种是经典的dropout方法，另一个是label smoothing方法，也就是在训练计算交叉熵时，不再使用one-hot向量而是将0处也填充上一个非0的极小值，可以一定程度上解决exposure bias的问题，增加模型的鲁棒性。</p>

              </div>
              <div class="toc-container">
                <ul class="markdownIt-TOC">
<li><a href="#transformer%E4%BB%8B%E7%BB%8D%E4%B8%AA%E4%BA%BA%E7%90%86%E8%A7%A3">Transformer介绍（个人理解）</a>
<ul>
<li><a href="#%E5%89%8D%E8%A8%80">前言</a></li>
<li><a href="#%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B">模型简介</a>
<ul>
<li><a href="#%E7%BC%96%E7%A0%81%E5%99%A8">编码器</a></li>
<li><a href="#%E8%A7%A3%E7%A0%81%E5%99%A8">解码器</a></li>
</ul>
</li>
<li><a href="#%E5%90%91%E9%87%8F%E5%8C%96">向量化</a></li>
<li><a href="#self-attention">Self-Attention</a>
<ul>
<li><a href="#self-attention%E4%BD%9C%E7%94%A8">Self-Attention作用</a></li>
<li><a href="#self-attention%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B">Self-Attention计算过程</a></li>
<li><a href="#self-attention%E5%A4%9A%E5%A4%B4%E6%9C%BA%E5%88%B6">Self-Attention多头机制</a></li>
<li><a href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81">位置编码</a></li>
</ul>
</li>
<li><a href="#%E6%AE%8B%E5%B7%AE">残差</a></li>
<li><a href="#%E8%A7%A3%E7%A0%81%E5%99%A8-2">解码器</a></li>
<li><a href="#%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95">优化算法</a></li>
<li><a href="#%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95">正则化方法</a></li>
</ul>
</li>
</ul>

              </div>
            </div>
          </article>
        </div>

        
          <div class="next-post">
            <div class="next">下一篇</div>
            <a href="https://classmate-xiaoming.github.io/post/hello-gridea/">
              <h3 class="post-title">
                Hello Gridea
              </h3>
            </a>
          </div>
        

        

        <div class="site-footer">
  Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a>
  <a class="rss" href="https://classmate-xiaoming.github.io/atom.xml" target="_blank">
    <i class="ri-rss-line"></i> RSS
  </a>
</div>

      </div>
    </div>

    <script>
      hljs.initHighlightingOnLoad()

      let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

      // This should probably be throttled.
      // Especially because it triggers during smooth scrolling.
      // https://lodash.com/docs/4.17.10#throttle
      // You could do like...
      // window.addEventListener("scroll", () => {
      //    _.throttle(doThatStuff, 100);
      // });
      // Only not doing it here to keep this Pen dependency-free.

      window.addEventListener("scroll", event => {
        let fromTop = window.scrollY;

        mainNavLinks.forEach((link, index) => {
          let section = document.getElementById(decodeURI(link.hash).substring(1));
          let nextSection = null
          if (mainNavLinks[index + 1]) {
            nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
          }
          if (section.offsetTop <= fromTop) {
            if (nextSection) {
              if (nextSection.offsetTop > fromTop) {
                link.classList.add("current");
              } else {
                link.classList.remove("current");    
              }
            } else {
              link.classList.add("current");
            }
          } else {
            link.classList.remove("current");
          }
        });
      });

    </script>
  </body>
</html>
